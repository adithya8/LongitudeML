{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Data setup for CLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datasets\n",
    "import torch\n",
    "from itertools import chain\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e172390184410aa3aa517551a5a10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597cf8844ba046e19c2c176ebb3a4d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/30.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0775823a69744d62974c919af49b3e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/21.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset tweet_eval/stance_abortion to /users2/avirinchipur/.cache/huggingface/datasets/tweet_eval/stance_abortion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5157aea2de4be9acc2276f7ee1a4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbcc912edb44dc28abc7a6c68df8096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/25.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6a49870fd5430aa292fd957337f5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed819ba1984f42c3a6d6eb0b21367379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b9e36205c74edd83d24a3956d84fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/107 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95447a7b75524b8d9404df44cf8588da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ce6da1b05e482f9f146ceccd1e9a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/61.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8418ea7214164c6d8ddc53578fe51c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c8b2c9fdf14649803b29bbf784991b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/587 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf9a0e04a454ccba6c3d14bbc87e36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45be28b38bf4826a7805b1ffe63f377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tweet_eval downloaded and prepared to /users2/avirinchipur/.cache/huggingface/datasets/tweet_eval/stance_abortion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b8eca665d049bfa93b7626aa537f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"tweet_eval\", \"stance_abortion\")\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label']\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['none', 'against', 'favor'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "print (raw_datasets['train'].column_names)\n",
    "print (raw_datasets['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'we remind ourselves that love means to be willing to give until it hurts - Mother Teresa',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3deafa57ddee40639da4fb95c76c9a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70b6a4d4812453c93c71d196f0a6b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162e7ec0178b42a5b95ea2b561ee855d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7efba6106e485bbec758f61d3aa9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3bde2fc12d4ff3b5bc7261f7b847ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb39c03148342caa893eae93b613806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_column_name = \"text\"\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=2,\n",
    "            remove_columns=raw_datasets[\"train\"].column_names,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print (tokenized_datasets['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  1195,\n",
       "  11484,\n",
       "  9655,\n",
       "  1115,\n",
       "  1567,\n",
       "  2086,\n",
       "  1106,\n",
       "  1129,\n",
       "  4988,\n",
       "  1106,\n",
       "  1660,\n",
       "  1235,\n",
       "  1122,\n",
       "  15483,\n",
       "  118,\n",
       "  4872,\n",
       "  12575,\n",
       "  102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  1195,\n",
       "  11484,\n",
       "  9655,\n",
       "  1115,\n",
       "  1567,\n",
       "  2086,\n",
       "  1106,\n",
       "  1129,\n",
       "  4988,\n",
       "  1106,\n",
       "  1660,\n",
       "  1235,\n",
       "  1122,\n",
       "  15483,\n",
       "  118,\n",
       "  4872,\n",
       "  12575,\n",
       "  102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = tokenized_datasets['train'][0:5]\n",
    "concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'we', 'remind', 'ourselves', 'that', 'love', 'means', 'to', 'be', 'willing', 'to', 'give', 'until', 'it', 'hurts', '-', 'Mother', 'Teresa', '[SEP]', '[CLS]', '@', 'user', '@', 'user', 'and', 'most', 'Islanders', 'have', 'different', 'definitions', 'of', '\"', 'time', '##ly', 'access', '\"', '.', '#', 'irony', '#', 'p', '##ei', '##poli', '#', 'p', '##ei', '##vo', '##tes', '[SEP]', '[CLS]', 'Life', 'is', '#', 'precious', '&', 'so', 'are', 'babies', ',', 'mothers', ',', '&', 'fathers', '.', 'Please', 'support', 'the', 'sa', '##nc', '##ti', '##ty', 'of', 'Human', 'Life', '.', 'Think', '#', 'Se', '##m', '##ST', '[SEP]', '[CLS]', '@', 'user', 'too', 'many', 'people', 'are', 'taking', 'this', 'to', 'seriously', '#', 'Se', '##m', '##ST', '[SEP]', '[CLS]', 'Dude', 'i', 'won', 'a', '#', 'free', '##shirt', 'from', '@', 'user', '!', 'I', 'never', 'win', 'anything', 'lo', '##l', '#', 'A', '##bor', '##t', '##7', '##3', '#', 'Se', '##m', '##ST', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print (tokenizer.convert_ids_to_tokens(concatenated_examples['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size=128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "    total_length = max((total_length // block_size) * block_size, 1)\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c537bcbf82884947857b0a865552526f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 128 #1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b69c47076b4eb3943f9485e16d94b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 128 #0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d062727146d4445aa3459df7b587531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 128 #1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d39cef15ff43c2a0887458aa0d7059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 128 #0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa49c980e4e34528bf973cfe5160ed87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 128 #1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa0c4444c4c4228b157c979e8ddb6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 128 #0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            num_proc=2,\n",
    "            desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 19)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_datasets['train'][0]['input_ids']), len(tokenized_datasets['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147, 587)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_datasets['train']), len(tokenized_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "        lm_datasets['train'], shuffle=False, collate_fn=default_data_collator, batch_size=16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first batch from the dataloader \n",
    "first_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1195, 11484,  ...,   102,   101,  2409],\n",
       "         [11078,   112,  1325,  ...,  1135,   112,   188],\n",
       "         [ 1136,  1139,  3026,  ...,   101,   160,  3048],\n",
       "         ...,\n",
       "         [  136,  3298,  4915,  ..., 16397,  1221,  1115],\n",
       "         [ 1297,  3471,  1120,  ...,   119,  1195,   112],\n",
       "         [ 1231,  1280,  1111,  ..., 10373,  2066,  1176]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': tensor([[  101,  1195, 11484,  ...,   102,   101,  2409],\n",
       "         [11078,   112,  1325,  ...,  1135,   112,   188],\n",
       "         [ 1136,  1139,  3026,  ...,   101,   160,  3048],\n",
       "         ...,\n",
       "         [  136,  3298,  4915,  ..., 16397,  1221,  1115],\n",
       "         [ 1297,  3471,  1120,  ...,   119,  1195,   112],\n",
       "         [ 1231,  1280,  1111,  ..., 10373,  2066,  1176]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fd50a1c681ff73ef\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /users2/avirinchipur/.cache/huggingface/datasets/generator/default-fd50a1c681ff73ef/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49dc7e4b83f54961ba385ffc683b7cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /users2/avirinchipur/.cache/huggingface/datasets/generator/default-fd50a1c681ff73ef/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "def gen():\n",
    "    yield {\"text\": \"Good\", \"answer\":{\"label\": 0}}\n",
    "    yield {\"text\": \"Bad\", \"answer\": {\"label\": 1}}\n",
    "    yield {\"text\": \"Ugly\", \"answer\": {\"label\": 2}}\n",
    "    yield {\"text\": \"Marvelous\", \"answer\":{\"label\": 3}}\n",
    "    \n",
    "ds = Dataset.from_generator(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'answer'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(ds, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Good', 'Ugly'], 'answer': {'label': tensor([0, 2])}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dict dataset into Huggingface Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 8]) torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 4\n",
    "BATCH_SEQ_LEN = 10\n",
    "FEAT_DIM = 8\n",
    "SEQ_LENS = torch.tensor([6, 3, 5, 10], dtype=torch.long)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "embeddings = torch.randn(NUM_EXAMPLES, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "mask = torch.nn.utils.rnn.pad_sequence([torch.ones(seq_len) for seq_len in SEQ_LENS], batch_first=True)\n",
    "print (embeddings.shape, mask.shape)\n",
    "labels_1 = torch.randint(0, 2, (NUM_EXAMPLES, ), dtype=torch.long)\n",
    "labels_2 = torch.randint(0, 2, (NUM_EXAMPLES, BATCH_SEQ_LEN), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "                'embedding':[embeddings[i].unsqueeze(0) for i in range(len(embeddings))], \n",
    "                'label':[labels_1[i] for i in range(len(labels_1))], \n",
    "                'id':[i for i in range(len(embeddings))]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['embedding', 'label', 'id'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'label': Value(dtype='int64', id=None),\n",
       " 'id': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, val and test ses into Dataset_dict object with embeddings, cls and label features\n",
    "# Write map functions to be able to set the data up for different kinds of training strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new = ds.train_test_split(test_size=0.2, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['embedding', 'label', 'id'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['embedding', 'label', 'id'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Sequence, ClassLabel, Features\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import FloatTensor, IntTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(train_data:dict, val_data:dict, test_data:dict):\n",
    "    \"\"\"\n",
    "        Returns the Huggingface datasets.dataset .\n",
    "    \"\"\"\n",
    "    # TODO: Add support to infill default vector/tokens for missing timesteps\n",
    "    dataset_obj = DatasetDict()\n",
    "    dataset_obj['train'] = Dataset.from_dict(train_data)\n",
    "    dataset_obj['dev'] = Dataset.from_dict(val_data)\n",
    "    dataset_obj['test'] = Dataset.from_dict(test_data)\n",
    "    return dataset_obj\n",
    "\n",
    "\n",
    "def create_mask(examples):\n",
    "    \"\"\"\n",
    "        Function that goes into DatasetDict.map() to create a mask pattern for the sequence\n",
    "        This function is for applying MIL for a single label representing the sequence\n",
    "    \"\"\"\n",
    "    def infill_missing_vector(instance):\n",
    "        \"\"\"\n",
    "            Infills missing vector with the previous vector\n",
    "            TODO: Other choices include default vector, neighbour average, learnable embedding vector\n",
    "        \"\"\"\n",
    "        sorted_seq_nums = sorted(instance['seq_num'])\n",
    "        missing_seq_nums = set(range(sorted_seq_nums[0], sorted_seq_nums[-1]+1)) - set(sorted_seq_nums)\n",
    "        for seq_num in missing_seq_nums:\n",
    "            instance['embeddings'][0].insert(seq_num, instance['embeddings'][0][seq_num-1])\n",
    "            instance['seq_num'].insert(seq_num, seq_num)\n",
    "        instance['labels'] = torch.tensor(instance['labels']).expand(1, len(instance['seq_num'])).tolist()\n",
    "            # if 'cls' in instance:\n",
    "                # instance['cls'][0].insert(seq_num, instance['cls'][0][seq_num-1])\n",
    "        return instance\n",
    "\n",
    "    def create_mask_pattern(instance):\n",
    "        \"\"\"\n",
    "            Creates a mask pattern for the sequence\n",
    "        \"\"\"\n",
    "        instance['mask'] = [[1]*len(instance['seq_num'])]# [1 if seq_num in instance['seq_num'] else 0 for seq_num in range(max(instance['seq_num'])+1)]\n",
    "        return instance\n",
    "    \n",
    "    examples = infill_missing_vector(examples)\n",
    "    examples = create_mask_pattern(examples)\n",
    "    \n",
    "    return examples  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 4\n",
    "BATCH_SEQ_LEN = 10\n",
    "FEAT_DIM = 8\n",
    "SEQ_LENS = torch.tensor([6, 3, 5, 10], dtype=torch.long)\n",
    "torch.manual_seed(420)\n",
    "\n",
    "embeddings_train = [torch.randn(1, SEQ_LENS[i], FEAT_DIM) for i in range(NUM_EXAMPLES)]\n",
    "labels_train = [torch.randint(0, 2, (1, 1), dtype=torch.long) for i in range(NUM_EXAMPLES)]\n",
    "seq_nums_train = [torch.randperm(BATCH_SEQ_LEN)[:SEQ_LENS[i]].sort()[0] for i in range(NUM_EXAMPLES)]\n",
    "seq_nums_train = [seq - seq[0] for seq in seq_nums_train]\n",
    "# seq_nums_train = [torch.arange(SEQ_LENS[i], dtype=torch.long) for i in range(NUM_EXAMPLES)]\n",
    "\n",
    "data_dict = dict(embeddings=embeddings_train, labels=labels_train, seq_num=seq_nums_train)\n",
    "dataset = get_dataset(data_dict, data_dict, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4284db126c14ed680d940c468f87f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbcc77b815f4bf4b8c743b8987482fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7176b8c1b9484cb0ee251f5c5165cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_masked = dataset.map(create_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of 0-th sequnce: 9\n",
      "Shape of its embeddings object: torch.Size([1, 9, 8])\n",
      "Shape its mask object: torch.Size([1, 9])\n",
      "Shape of its labels object: torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print (\"Len of {}-th sequnce: {}\".format(i, dataset_masked['train'][i]['seq_num'].__len__()))\n",
    "print (\"Shape of its embeddings object: {}\".format(torch.tensor(dataset_masked['train'][0]['embeddings']).shape))\n",
    "print (\"Shape its mask object: {}\".format(torch.tensor(dataset_masked['train'][0]['mask']).shape))\n",
    "print (\"Shape of its labels object: {}\".format(torch.tensor(dataset_masked['train'][0]['labels']).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'seq_num': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'mask': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_masked['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 9, 8]), torch.Size([1, 9]), torch.Size([9]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset_masked['train']['embeddings'][0]).shape, torch.tensor(dataset_masked['train']['labels'][0]).shape, torch.tensor(dataset_masked['train']['seq_num'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "for feat in dataset_masked['train']:\n",
    "    print (feat['seq_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(features):\n",
    "    # Features dict have embeddings, label, seq_num of single sequence\n",
    "    # Embeddings shape: (1, seq_len, hidden_dim)\n",
    "    # Labels shape: (1, seq_len)\n",
    "    # seq_num shape: (seq_len, )\n",
    "    first = features[0]\n",
    "    batch = {}\n",
    "    max_seq_len = max([feat['seq_num'][-1] for feat in features])+1\n",
    "    for k, _ in first.items():\n",
    "        if k == \"embeddings\":\n",
    "            for feat in features:\n",
    "                embeddings = torch.tensor(feat['embeddings']).clone()\n",
    "                if embeddings.shape[1] < max_seq_len:\n",
    "                    zeros = torch.zeros((1, max_seq_len - embeddings.shape[1], embeddings.shape[2]))\n",
    "                    feat['embeddings'] = torch.cat((embeddings, zeros), dim=1)\n",
    "            batch[k] = torch.cat([torch.tensor(f[k]) for f in features], dim=0)\n",
    "        elif k == \"seq_num\":\n",
    "            batch[k] = torch.nn.utils.rnn.pad_sequence([torch.tensor(f[k]) for f in features], padding_value=-1, batch_first=True)\n",
    "        elif k == \"labels\":\n",
    "            batch[k] = torch.nn.utils.rnn.pad_sequence([torch.tensor(f[k]).squeeze(0) for f in features], padding_value=-1, batch_first=True)\n",
    "    return batch\n",
    "\n",
    "dataloader = DataLoader(dataset_masked['train'], batch_size=2, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_438/362737656.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[k] = torch.cat([torch.tensor(f[k]) for f in features], dim=0)\n"
     ]
    }
   ],
   "source": [
    "nxt = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['embeddings', 'labels', 'seq_num'])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 8])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxt['embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7, -1],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxt['seq_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
       "        [ 1,  1,  1,  1,  1,  1,  1,  1,  1]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxt['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7281, -0.7106, -0.6021,  0.9604,  0.4048, -1.3543, -0.4976,  0.4747],\n",
       "        [-0.1976,  1.2683,  1.2243,  0.0981,  1.7423, -1.3527,  0.2191,  0.5526],\n",
       "        [-0.6788,  0.5743,  0.1877, -0.3576, -0.3165,  0.5886, -0.8905,  0.4098],\n",
       "        [ 1.9312,  1.0119, -1.4364, -1.1299, -0.1360,  1.6354,  0.6547,  0.5760],\n",
       "        [ 1.1415,  0.0186, -1.8058,  0.9254, -0.3753,  1.0331, -0.6867,  0.6368],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxt['embeddings'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxt['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fix the dimensions of the mask, seq_num and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 8])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([embeddings[:1], torch.zeros(1, 2, 8)], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
