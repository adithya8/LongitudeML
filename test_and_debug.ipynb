{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the dataset module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import get_datasetDict, create_mask, default_collate_fn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 6\n",
    "BATCH_SEQ_LEN = 10\n",
    "FEAT_DIM = 8\n",
    "SEQ_LENS_TRAIN = torch.tensor([6, 3, 5, 10, 4, 2], dtype=torch.long)\n",
    "SEQ_LENS_VAL = torch.tensor([4, 2, 7, 8, 9, 3], dtype=torch.long)\n",
    "SEQ_LENS_TEST = torch.tensor([5, 3, 6, 9, 5, 7], dtype=torch.long)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "embeddings_train = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "labels_train = torch.randint(0, 2, (BATCH_SIZE, ), dtype=torch.long).unsqueeze(-1).expand(BATCH_SIZE, BATCH_SEQ_LEN)\n",
    "seq_lens = [list(range(SEQ_LENS_TRAIN[i])) for i in range(BATCH_SIZE)]\n",
    "embeddings_train = [embeddings[:, :SEQ_LENS_TRAIN[i]] for i, embeddings in enumerate(embeddings_train)]\n",
    "train_data = {'embeddings': embeddings_train, 'labels': labels_train, 'seq_num': seq_lens}\n",
    "\n",
    "embeddings_val = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "labels_val = torch.randint(0, 2, (BATCH_SIZE, ), dtype=torch.long).unsqueeze(-1).expand(BATCH_SIZE, BATCH_SEQ_LEN)\n",
    "seq_lens = [list(range(SEQ_LENS_VAL[i])) for i in range(BATCH_SIZE)]\n",
    "embeddings_val = [embeddings[:, :SEQ_LENS_VAL[i]] for i, embeddings in enumerate(embeddings_val)]\n",
    "val_data = {'embeddings': embeddings_val, 'labels': labels_val, 'seq_num': seq_lens}\n",
    "\n",
    "embeddings_test = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "labels_test = torch.randint(0, 2, (BATCH_SIZE, ), dtype=torch.long).unsqueeze(-1).expand(BATCH_SIZE, BATCH_SEQ_LEN)\n",
    "seq_lens = [list(range(SEQ_LENS_TEST[i])) for i in range(BATCH_SIZE)]\n",
    "embeddings_test = [embeddings[:, :SEQ_LENS_TEST[i]] for i, embeddings in enumerate(embeddings_test)]\n",
    "test_data = {'embeddings': embeddings_test, 'labels': labels_test, 'seq_num': seq_lens}\n",
    "\n",
    "datasetDict = get_datasetDict(train_data=train_data, val_data=val_data, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 8])\n",
      "torch.Size([1, 3, 8])\n",
      "torch.Size([1, 5, 8])\n",
      "torch.Size([1, 10, 8])\n",
      "torch.Size([1, 4, 8])\n",
      "torch.Size([1, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "embeddings_train = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "for i, embeddings in enumerate(embeddings_train):\n",
    "    print (embeddings[:, :SEQ_LENS_TRAIN[i]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 8])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['embeddings', 'labels', 'seq_num'],\n",
       "    num_rows: 6\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetDict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 8]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(datasetDict['train'][5]['embeddings']).shape, torch.tensor(datasetDict['train'][5]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1c43358f534844bb267bd41342c257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783e9ff5187742ccb5b530699c6588a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029a69ec26a74c688d31392ea0e76b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasetDict_mask = datasetDict.map(create_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1],\n",
       " [1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetDict_mask['train'][0:4]['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(datasetDict_mask['train'], batch_size=3, shuffle=False, collate_fn=default_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users2/avirinchipur/NLP/emi/src/mi_datamodule.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[k] = torch.cat([torch.tensor(f[k]) for f in features], dim=0)\n"
     ]
    }
   ],
   "source": [
    "nxt = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetDict_mask['train'][0]['seq_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 8])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxt['embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxt['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxt['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5],\n",
       "        [0, 1, 2, 0, 0, 0],\n",
       "        [0, 1, 2, 3, 4, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxt['seq_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9006,  0.2286,  0.0249, -0.3460,  0.2868, -0.7308,  0.1748, -1.0939],\n",
       "        [-1.6022,  1.3529,  1.2888,  0.0523, -1.5469,  0.7567,  0.7755,  2.0265],\n",
       "        [ 0.0358,  0.1206, -0.8057, -0.2076, -0.9319, -1.5910, -1.1360, -0.5226],\n",
       "        [-0.5188, -1.5013, -1.9267,  0.1279,  1.0229, -0.5558,  0.7043,  0.7099],\n",
       "        [ 1.7744, -0.9216,  0.9624, -0.3370, -1.1753,  0.3581,  0.4788,  1.3537],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxt['embeddings'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9968e+11,  4.5695e-41,  3.5287e-19],\n",
       "        [ 3.0740e-41,  4.4842e-44,  0.0000e+00],\n",
       "        [ 1.5695e-43,  0.0000e+00,  3.4970e-19]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3,3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing MIDataLoaderModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src import get_default_args\n",
    "from src import get_datasetDict, create_mask, default_collate_fn, recurrent\n",
    "from src import MIDataLoaderModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 6\n",
    "BATCH_SEQ_LEN = 10\n",
    "FEAT_DIM = 8\n",
    "SEQ_LENS_TRAIN = torch.tensor([6, 3, 5, 10, 4, 2], dtype=torch.long)\n",
    "SEQ_LENS_VAL = torch.tensor([4, 2, 7, 8, 9, 3], dtype=torch.long)\n",
    "SEQ_LENS_TEST = torch.tensor([5, 3, 6, 9, 5, 7], dtype=torch.long)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "embeddings_train = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "labels_train = torch.randint(0, 2, (BATCH_SIZE, ), dtype=torch.long).unsqueeze(-1).expand(BATCH_SIZE, BATCH_SEQ_LEN)\n",
    "seq_lens = [list(range(SEQ_LENS_TRAIN[i])) for i in range(BATCH_SIZE)]\n",
    "embeddings_train = [embeddings[:, :SEQ_LENS_TRAIN[i]] for i, embeddings in enumerate(embeddings_train)]\n",
    "train_data = {'embeddings': embeddings_train, 'labels': labels_train, 'seq_num': seq_lens}\n",
    "\n",
    "embeddings_val = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "labels_val = torch.randint(0, 2, (BATCH_SIZE, ), dtype=torch.long).unsqueeze(-1).expand(BATCH_SIZE, BATCH_SEQ_LEN)\n",
    "seq_lens = [list(range(SEQ_LENS_VAL[i])) for i in range(BATCH_SIZE)]\n",
    "embeddings_val = [embeddings[:, :SEQ_LENS_VAL[i]] for i, embeddings in enumerate(embeddings_val)]\n",
    "val_data = {'embeddings': embeddings_val, 'labels': labels_val, 'seq_num': seq_lens}\n",
    "\n",
    "embeddings_test = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "labels_test = torch.randint(0, 2, (BATCH_SIZE, ), dtype=torch.long).unsqueeze(-1).expand(BATCH_SIZE, BATCH_SEQ_LEN)\n",
    "seq_lens = [list(range(SEQ_LENS_TEST[i])) for i in range(BATCH_SIZE)]\n",
    "embeddings_test = [embeddings[:, :SEQ_LENS_TEST[i]] for i, embeddings in enumerate(embeddings_test)]\n",
    "test_data = {'embeddings': embeddings_test, 'labels': labels_test, 'seq_num': seq_lens}\n",
    "\n",
    "datasetDict = get_datasetDict(train_data=train_data, val_data=val_data, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99a399041bd4c7ab60a32196c3fa9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdebe47428946d7aef7a2c86d5595e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042deb7aa4764a438b5452618ac95d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasetDict_mask = datasetDict.map(create_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = MIDataLoaderModule(data_args=get_default_args(jupyter=True), datasets=datasetDict_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = dataloader.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users2/avirinchipur/NLP/emi/src/mi_datamodule.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[k] = torch.cat([torch.tensor(f[k]) for f in features], dim=0)\n"
     ]
    }
   ],
   "source": [
    "a = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': tensor([[[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431,\n",
       "           -1.6047],\n",
       "          [-0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594, -0.7688,\n",
       "            0.7624],\n",
       "          [ 1.6423, -0.1596, -0.4974,  0.4396, -0.7581,  1.0783,  0.8008,\n",
       "            1.6806],\n",
       "          [ 1.2791,  1.2964,  0.6105,  1.3347, -0.2316,  0.0418, -0.2516,\n",
       "            0.8599],\n",
       "          [-1.3847, -0.8712, -0.2234,  1.7174,  0.3189, -0.4245,  0.3057,\n",
       "           -0.7746],\n",
       "          [-1.5576,  0.9956, -0.8798, -0.6011, -1.2742,  2.1228, -1.2347,\n",
       "           -0.4879],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000]],\n",
       " \n",
       "         [[ 0.0109, -0.3387, -1.3407, -0.5854,  0.5362,  0.5246,  1.1412,\n",
       "            0.0516],\n",
       "          [ 0.7440, -0.4816, -1.0495,  0.6039, -1.7223, -0.8278,  1.3347,\n",
       "            0.4835],\n",
       "          [-2.5095,  0.4880,  0.7846,  0.0286,  0.6408,  0.5832,  1.0669,\n",
       "           -0.4502],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000]],\n",
       " \n",
       "         [[-1.9006,  0.2286,  0.0249, -0.3460,  0.2868, -0.7308,  0.1748,\n",
       "           -1.0939],\n",
       "          [-1.6022,  1.3529,  1.2888,  0.0523, -1.5469,  0.7567,  0.7755,\n",
       "            2.0265],\n",
       "          [ 0.0358,  0.1206, -0.8057, -0.2076, -0.9319, -1.5910, -1.1360,\n",
       "           -0.5226],\n",
       "          [-0.5188, -1.5013, -1.9267,  0.1279,  1.0229, -0.5558,  0.7043,\n",
       "            0.7099],\n",
       "          [ 1.7744, -0.9216,  0.9624, -0.3370, -1.1753,  0.3581,  0.4788,\n",
       "            1.3537],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000]],\n",
       " \n",
       "         [[-0.9291,  0.2762, -0.5389,  0.4626, -0.8719, -0.0271, -0.3532,\n",
       "            1.4639],\n",
       "          [ 1.2554, -0.7150,  0.8539,  0.5130,  0.5397,  0.5655,  0.5058,\n",
       "            0.2225],\n",
       "          [-0.6855,  0.5636, -1.5072, -1.6107, -1.4790,  0.4323, -0.1250,\n",
       "            0.7821],\n",
       "          [-1.5988, -0.1091,  0.7152,  0.0391,  1.3059,  0.2466, -1.9776,\n",
       "            0.0179],\n",
       "          [-1.3793,  0.6258, -2.5850, -0.0240, -0.1222, -0.7470,  1.7093,\n",
       "            0.0579],\n",
       "          [ 1.1930,  1.9373,  0.7287,  0.9809,  0.4146,  1.1566,  0.2691,\n",
       "           -0.0366],\n",
       "          [ 0.9733, -1.0151, -0.5419, -0.4410, -0.3136, -0.1293, -0.7150,\n",
       "           -0.0476],\n",
       "          [ 2.0207,  0.2539,  0.9364,  0.7122, -0.0318,  0.1016,  1.3433,\n",
       "            0.7133],\n",
       "          [ 0.4038, -0.7140,  0.8337, -0.9585,  0.4536,  1.2461, -2.3065,\n",
       "           -1.2869],\n",
       "          [ 0.1799, -2.1268, -0.1341, -1.0408, -0.7647, -0.0553,  1.2049,\n",
       "           -0.9825]],\n",
       " \n",
       "         [[ 0.4334, -0.7172,  1.0554, -1.4534,  0.4652,  0.3714, -0.0047,\n",
       "            0.0795],\n",
       "          [ 0.3782,  0.7051, -1.7237, -0.8435,  0.4351,  0.2659, -0.5871,\n",
       "            0.0827],\n",
       "          [ 0.8854,  0.1824,  0.7864, -0.0579,  0.5667, -0.7098, -0.4875,\n",
       "            0.0501],\n",
       "          [ 0.6084,  1.6309, -0.0847,  1.0844,  0.9478, -0.6766, -0.5730,\n",
       "           -0.3303],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000]],\n",
       " \n",
       "         [[ 0.0247, -1.0641, -0.7602, -0.4075,  0.9624, -0.1426,  0.1527,\n",
       "           -0.0388],\n",
       "          [ 0.9446, -1.5824,  0.9871,  1.1457, -0.1418, -0.2763, -0.1932,\n",
       "            0.7768],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000]]]),\n",
       " 'labels': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
       " 'seq_num': tensor([[0, 1, 2, 3, 4, 5, 0, 0, 0, 0],\n",
       "         [0, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 2, 3, 4, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "         [0, 1, 2, 3, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'mask': tensor([[ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False, False, False]])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src import get_datasetDict, create_mask, default_collate_fn, recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 6\n",
    "BATCH_SEQ_LEN = 10\n",
    "FEAT_DIM = 8\n",
    "SEQ_LENS_TRAIN = torch.tensor([6, 3, 5, 10, 4, 2], dtype=torch.long)\n",
    "SEQ_LENS_VAL = torch.tensor([4, 2, 7, 8, 9, 3], dtype=torch.long)\n",
    "SEQ_LENS_TEST = torch.tensor([5, 3, 6, 9, 5, 7], dtype=torch.long)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "embeddings_train = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "labels_train = torch.randint(0, 2, (BATCH_SIZE, ), dtype=torch.long).unsqueeze(-1).expand(BATCH_SIZE, BATCH_SEQ_LEN)\n",
    "seq_lens = [list(range(SEQ_LENS_TRAIN[i])) for i in range(BATCH_SIZE)]\n",
    "embeddings_train = [embeddings[:, :SEQ_LENS_TRAIN[i]] for i, embeddings in enumerate(embeddings_train)]\n",
    "train_data = {'embeddings': embeddings_train, 'labels': labels_train, 'seq_num': seq_lens}\n",
    "\n",
    "embeddings_val = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "labels_val = torch.randint(0, 2, (BATCH_SIZE, ), dtype=torch.long).unsqueeze(-1).expand(BATCH_SIZE, BATCH_SEQ_LEN)\n",
    "seq_lens = [list(range(SEQ_LENS_VAL[i])) for i in range(BATCH_SIZE)]\n",
    "embeddings_val = [embeddings[:, :SEQ_LENS_VAL[i]] for i, embeddings in enumerate(embeddings_val)]\n",
    "val_data = {'embeddings': embeddings_val, 'labels': labels_val, 'seq_num': seq_lens}\n",
    "\n",
    "embeddings_test = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "labels_test = torch.randint(0, 2, (BATCH_SIZE, ), dtype=torch.long).unsqueeze(-1).expand(BATCH_SIZE, BATCH_SEQ_LEN)\n",
    "seq_lens = [list(range(SEQ_LENS_TEST[i])) for i in range(BATCH_SIZE)]\n",
    "embeddings_test = [embeddings[:, :SEQ_LENS_TEST[i]] for i, embeddings in enumerate(embeddings_test)]\n",
    "test_data = {'embeddings': embeddings_test, 'labels': labels_test, 'seq_num': seq_lens}\n",
    "\n",
    "datasetDict = get_datasetDict(train_data=train_data, val_data=val_data, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72977968fb97444b93aaec7a42231be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa98d463d9e0453cb2ad74eb6feb05c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48dc9ed541c4f639b22f52419f3dbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasetDict_mask = datasetDict.map(create_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(datasetDict_mask['train'], batch_size=3, shuffle=False, collate_fn=default_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = recurrent(input_size=8, hidden_size=4, bidirectional=False, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users2/avirinchipur/NLP/emi/src/mi_datamodule.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[k] = torch.cat([torch.tensor(f[k]) for f in features], dim=0)\n"
     ]
    }
   ],
   "source": [
    "input_data = next(iter(train_dataloader))\n",
    "input_rep, mask = input_data['embeddings'], input_data['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 6, 8]), torch.Size([3, 6]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_rep.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(embeddings=input_rep, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(embeddings=input_rep, mask=mask, predict_last_valid_hidden_state=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from src import (\n",
    "    get_default_args, get_logger,\n",
    "    get_datasetDict, create_mask, MIDataLoaderModule,\n",
    "    MILightningModule\n",
    ")\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Session.key': 'b\"1eaf328c-50b5-44d9-85ec-a942a771cf26\"',\n",
      " 'Session.signature_scheme': '\"hmac-sha256\"',\n",
      " 'bidirectional': False,\n",
      " 'control': '9006',\n",
      " 'cross_entropy_class_weight': None,\n",
      " 'data_dir': None,\n",
      " 'dev_file': None,\n",
      " 'dropout': 0.0,\n",
      " 'epochs': 10,\n",
      " 'eval_batch_size': 64,\n",
      " 'f': '/users2/avirinchipur/.local/share/jupyter/runtime/kernel-v2-321021dNdl6Oi52YP.json',\n",
      " 'hb': '9005',\n",
      " 'hidden_size': 128,\n",
      " 'input_size': 8,\n",
      " 'iopub': '9009',\n",
      " 'ip': '127.0.0.1',\n",
      " 'log_interval': 10,\n",
      " 'lr': 0.001,\n",
      " 'model': 'gru',\n",
      " 'num_classes': 2,\n",
      " 'num_layers': 1,\n",
      " 'num_workers': 4,\n",
      " 'output_dir': None,\n",
      " 'overwrite_output_dir': False,\n",
      " 'predict_last_valid_timestep': False,\n",
      " 'save_dir': None,\n",
      " 'save_strategy': 'best',\n",
      " 'seed': 42,\n",
      " 'shell': '9007',\n",
      " 'stdin': '9008',\n",
      " 'test_file': None,\n",
      " 'train_batch_size': 32,\n",
      " 'train_file': None,\n",
      " 'transport': '\"tcp\"',\n",
      " 'weight_decay': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# get_data_args(parser)\n",
    "# get_model_args(parser)\n",
    "# get_training_args(parser)\n",
    "# args = parser.parse_args()\n",
    "args = get_default_args(jupyter=True)\n",
    "pprint (args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = get_logger(logger_name='wandb', project='emi-test-area', experiment='test-exp', name='test-name', prefix='test-prefix', save_dir='./lightning_logs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n"
     ]
    }
   ],
   "source": [
    "logger = get_logger(logger_name=\"comet\", workspace=\"adithya8\", project_name=\"test-project\", experiment_name=\"test-exp10\", save_dir=\"./lightning_logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m You are trying to log string value as a metric. This is not recommended.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/adithya8/test-project/0e31c682ab8f4019a1b1fed6edf1926e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.log_hyperparams(args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.output_dir = \"./lightning_logs/version_239/\"\n",
    "args.train_batch_size, args.eval_batch_size = 3, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Unknown error exporting current conda environment\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 6\n",
    "BATCH_SEQ_LEN = 10\n",
    "FEAT_DIM = 8\n",
    "SEQ_LENS_TRAIN = torch.tensor([6, 10, 5, 10, 4, 2], dtype=torch.long)\n",
    "SEQ_LENS_VAL = torch.tensor([4, 2, 10, 8, 9, 10], dtype=torch.long)\n",
    "SEQ_LENS_TEST = torch.tensor([5, 3, 10, 9, 5, 10], dtype=torch.long)\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "embeddings_train = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "labels_train = torch.randint(0, 2, (BATCH_SIZE, ), dtype=torch.long).unsqueeze(-1).expand(BATCH_SIZE, BATCH_SEQ_LEN)\n",
    "seq_lens = [list(range(SEQ_LENS_TRAIN[i])) for i in range(BATCH_SIZE)]\n",
    "embeddings_train = [embeddings[:, :SEQ_LENS_TRAIN[i]] for i, embeddings in enumerate(embeddings_train)]\n",
    "train_data = {'embeddings': embeddings_train, 'labels': labels_train, 'seq_num': seq_lens}\n",
    "\n",
    "embeddings_val = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "labels_val = torch.randint(0, 2, (BATCH_SIZE, ), dtype=torch.long).unsqueeze(-1).expand(BATCH_SIZE, BATCH_SEQ_LEN)\n",
    "seq_lens = [list(range(SEQ_LENS_VAL[i])) for i in range(BATCH_SIZE)]\n",
    "embeddings_val = [embeddings[:, :SEQ_LENS_VAL[i]] for i, embeddings in enumerate(embeddings_val)]\n",
    "val_data = {'embeddings': embeddings_val, 'labels': labels_val, 'seq_num': seq_lens}\n",
    "\n",
    "embeddings_test = torch.randn(BATCH_SIZE, 1, BATCH_SEQ_LEN, FEAT_DIM)\n",
    "labels_test = torch.randint(0, 2, (BATCH_SIZE, ), dtype=torch.long).unsqueeze(-1).expand(BATCH_SIZE, BATCH_SEQ_LEN)\n",
    "seq_lens = [list(range(SEQ_LENS_TEST[i])) for i in range(BATCH_SIZE)]\n",
    "embeddings_test = [embeddings[:, :SEQ_LENS_TEST[i]] for i, embeddings in enumerate(embeddings_test)]\n",
    "test_data = {'embeddings': embeddings_test, 'labels': labels_test, 'seq_num': seq_lens}\n",
    "\n",
    "datasetDict = get_datasetDict(train_data=train_data, val_data=val_data, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64b58e9be9e458a85362082cbc85f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5309224829244fb98ce5edaf330e09cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73daa3eaf7b24011942ef050088521a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasetDict = datasetDict.map(create_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = MIDataLoaderModule(args, datasetDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=args.epochs, default_root_dir=args.output_dir, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_module = MILightningModule(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MILightningModule(\n",
       "  (model): recurrent(\n",
       "    (model): ModuleList(\n",
       "      (0): GRU(8, 128, batch_first=True)\n",
       "      (1): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | recurrent         | 53.1 K\n",
      "1 | loss  | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------\n",
      "53.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "53.1 K    Total params\n",
      "0.212     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9104004450435090885a66cefc4bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss epoch 0: 0.6935026049613953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/avirinchipur/conda_envs/emi/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/users2/avirinchipur/NLP/emi/src/mi_datamodule.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[k] = torch.cat([torch.tensor(f[k]) for f in features], dim=0)\n",
      "/data/avirinchipur/conda_envs/emi/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/data/avirinchipur/conda_envs/emi/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4a0a366fd84c449aedf792bcdb18c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6dce9c2b2be4290b5989f9ed5cde293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss epoch 0: 0.6939302086830139\n",
      "train loss epoch 0: 0.6911230087280273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27943130ca264763a3d3de9bee43ae78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss epoch 1: 0.694681704044342\n",
      "train loss epoch 1: 0.6774123907089233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a8962ae24e4571a65990be6a5d2ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss epoch 2: 0.6960177421569824\n",
      "train loss epoch 2: 0.6646286249160767\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede4c3d1f48f4ef6886cde06f30235ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss epoch 3: 0.6980670094490051\n",
      "train loss epoch 3: 0.6519855260848999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cbd834e2604517841fcb7497458ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss epoch 4: 0.7010512351989746\n",
      "train loss epoch 4: 0.6390383243560791\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fe4907402a42ce8b22d9e57167c4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss epoch 5: 0.705357551574707\n",
      "train loss epoch 5: 0.6253834962844849\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365c438aa8e441afbb6c0655ca12505b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss epoch 6: 0.7116540670394897\n",
      "train loss epoch 6: 0.6106442213058472\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1f6fef10584c7bba8c9c5c77dda0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss epoch 7: 0.7211464643478394\n",
      "train loss epoch 7: 0.5945127010345459\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49f7b410ff844fdba0641b97cd8af01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss epoch 8: 0.7361235022544861\n",
      "train loss epoch 8: 0.5769088864326477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061369ee480945f28e2eff7547acb6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss epoch 9: 0.7609699964523315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/adithya8/test-project/0e31c682ab8f4019a1b1fed6edf1926e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss epoch 9: 0.5583810210227966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_epoch_loss [10] : (0.5583810210227966, 0.6911230087280273)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [20]       : (0.5377485156059265, 0.691888689994812)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_epoch_loss [11]   : (0.6935026049613953, 0.7609699964523315)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_loss [22]         : (0.6552802324295044, 0.8666598200798035)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from : pytorch-lightning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name         : test-exp10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     bidirectional               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cross_entropy_class_weight  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data_dir                    : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dev_file                    : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout                     : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs                      : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_batch_size             : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hidden_size                 : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     input_size                  : 8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     log_interval                : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr                          : 0.001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model                       : gru\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_classes                 : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_layers                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_workers                 : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     output_dir                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     overwrite_output_dir        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     predict_last_valid_timestep : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_dir                    : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_strategy               : best\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seed                        : 42\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_file                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_batch_size            : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_file                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay                : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info               : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (33.12 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(lightning_module, datamodule=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/adithya8/test-project/d9fb41985e44481f8a96abf38edd240a\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml ExistingExperiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/adithya8/test-project/d9fb41985e44481f8a96abf38edd240a\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from : pytorch-lightning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name         : test-exp3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 1 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "logger.experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional.regression import pearson_corrcoef as pearson\n",
    "from torchmetrics.functional.regression import mean_absolute_error as mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [0.6943069696426392,\n",
       "  0.6897984147071838,\n",
       "  0.6854313015937805,\n",
       "  0.6811696887016296,\n",
       "  0.6769759058952332,\n",
       "  0.6728121042251587,\n",
       "  0.6686407923698425,\n",
       "  0.6644251942634583,\n",
       "  0.6601274609565735,\n",
       "  0.6557108163833618],\n",
       " 'val': [0.6947422623634338,\n",
       "  0.6961889863014221,\n",
       "  0.6977480053901672,\n",
       "  0.6994255185127258,\n",
       "  0.7012319564819336,\n",
       "  0.7031817436218262,\n",
       "  0.7052913904190063,\n",
       "  0.7075844407081604,\n",
       "  0.7100932002067566,\n",
       "  0.7128605246543884,\n",
       "  0.7159401178359985],\n",
       " 'test': []}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_module.epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dl = dataloader.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users2/avirinchipur/NLP/emi/src/mi_datamodule.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[k] = torch.cat([torch.tensor(f[k]) for f in features], dim=0)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(tr_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(batch['mask'], torch.BoolTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings torch.Size([6, 9, 8])\n",
      "labels torch.Size([6, 10])\n",
      "seq_num torch.Size([6, 9])\n",
      "mask torch.Size([6, 9])\n"
     ]
    }
   ],
   "source": [
    "for k, v in batch.items():\n",
    "    print (k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
